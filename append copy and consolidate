def export_consolidated_results(self):
    """
    Export consolidated results - FIXED to preserve RAG sheets
    """
    try:
        temp_df = None
        today = datetime.now()
        today_str = today.strftime("%m_%d_%y")
        csv_files = glob.glob(f"{self.temp_path}/*.csv")
        sheet_name = "Consolidated_Report"
        consolidated_file = f"{self.out_path}/{today_str}_{sheet_name}.xlsx"
        lock_path = consolidated_file + '.lock'
        ws_rows = []
        
        # Use a lock so only one process writes at a time
        with FileLock(lock_path, timeout=60):
            for file in csv_files:
                base_name = os.path.basename(file)
                trade_id = base_name.split("_")[0]
                rows_found = False
                
                with open(file, newline="", encoding="utf-8") as f:
                    reader = csv.DictReader(f)
                    csv_headers = list(reader.fieldnames) if reader.fieldnames else []
                    expected_headers = self.static_headers[1:]
                    required_sub_headers = self.static_headers[19:]
                    
                    if all(header in csv_headers for header in required_sub_headers):
                        for row in reader:
                            out_row = [trade_id]
                            for header in expected_headers:
                                out_row.append(row.get(header, ""))
                            ws_rows.append(out_row)
                            rows_found = True
                    
                    elif any(header in csv_headers for header in expected_headers[1:]):
                        for row in reader:
                            out_row = [trade_id]
                            for header in expected_headers:
                                out_row.append(row.get(header, ""))
                            ws_rows.append(out_row)
                            rows_found = True
                    
                    else:
                        if len(csv_headers) == 0:
                            out_row = [f"{trade_id} - Found in Mail - No Entity Found"] + [""] * (len(self.static_headers) - 1)
                        else:
                            out_row = [f"{trade_id} - Partial header mismatch - Check file: {base_name}"] + [""] * (len(self.static_headers) - 1)
                        ws_rows.append(out_row)
                        rows_found = True
                    
                    f.close()
                
                if not rows_found:
                    out_row = [f"{trade_id} - Not Found in Mail"] + [""] * (len(self.static_headers) - 1)
                    ws_rows.append(out_row)
                    os.remove(file)
            
            # Build dataframe for new trade data
            df_new = pd.DataFrame(ws_rows, columns=self.static_headers)
            
            # ===== FIX: PRESERVE EXISTING RAG SHEETS =====
            if os.path.isfile(consolidated_file):
                # File exists - need to preserve RAG sheets
                logger.info(f"Consolidated file exists - preserving RAG sheets")
                
                # Read existing Sheet1 data
                df_existing = pd.read_excel(consolidated_file, sheet_name=sheet_name)
                df_combined = pd.concat([df_existing, df_new], ignore_index=True)
                df_combined = df_combined.drop_duplicates()
                
                # Load workbook to preserve other sheets
                wb = load_workbook(consolidated_file)
                
                # Save all sheet names EXCEPT Sheet1
                other_sheets = {}
                for ws_name in wb.sheetnames:
                    if ws_name != sheet_name:
                        # Read data from other sheets
                        ws = wb[ws_name]
                        sheet_data = []
                        for row in ws.iter_rows(values_only=True):
                            sheet_data.append(row)
                        other_sheets[ws_name] = sheet_data
                
                wb.close()
                
                # Write Sheet1 with updated data
                with pd.ExcelWriter(consolidated_file, engine='openpyxl', mode='w') as writer:
                    df_combined.to_excel(writer, sheet_name=sheet_name, index=False)
                
                # Restore other sheets
                if other_sheets:
                    wb = load_workbook(consolidated_file)
                    for ws_name, sheet_data in other_sheets.items():
                        if ws_name not in wb.sheetnames:
                            ws = wb.create_sheet(ws_name)
                        else:
                            ws = wb[ws_name]
                        
                        # Write data back
                        for row_idx, row in enumerate(sheet_data, 1):
                            for col_idx, value in enumerate(row, 1):
                                ws.cell(row=row_idx, column=col_idx, value=value)
                    
                    wb.save(consolidated_file)
                    wb.close()
                    logger.info(f"✓ Preserved {len(other_sheets)} RAG sheets")
            
            else:
                # New file - just write Sheet1
                df_new.to_excel(consolidated_file, sheet_name=sheet_name, index=False)
                logger.info("✓ Created new consolidated file")
            
            # Display summary
            temp_df = pd.read_excel(consolidated_file, sheet_name=sheet_name)
            if not temp_df.empty:
                self.entity_result_text.delete(1.0, tk.END)
                self.entity_result_text.insert(tk.END, f"{temp_df}\n")
            else:
                self.entity_result_text.insert(tk.END, "No Data found in this email")
        
        logger.info("Success - Consolidated Report File Generated")
        
        # ===== NOW APPEND RAG DATA (SHEETS ALREADY PRESERVED) =====
        self._append_rag_to_consolidated(consolidated_file)
        
        # Copy files and cleanup
        self.copy_files(self.temp_path, self.out_path)
        
        messagebox.showinfo("Success", f"Consolidated Report File Generated: {self.out_path}")
    
    except Exception as e:
        logger.info(f"Export Error Failed to export : {e}")
        messagebox.showerror("Export Error", f"Failed to export : {e}")











def _append_rag_to_consolidated(self, consolidated_file: str):
    """
    Append RAG report data to consolidated file (Sheet2+)
    FIX: Properly handle existing sheets and append without recreating
    """
    try:
        # Find latest RAG report in temp_path
        rag_files = glob.glob(f"{self.temp_path}/RAG_Pattern_Analysis_*.xlsx")
        
        if not rag_files:
            logger.info("No RAG report found - skipping RAG consolidation")
            return
        
        # Get most recent RAG file
        latest_rag = max(rag_files, key=os.path.getctime)
        logger.info(f"Found RAG report: {os.path.basename(latest_rag)}")
        
        # Extract timestamp from filename
        rag_filename = os.path.basename(latest_rag)
        timestamp = rag_filename.replace("RAG_Pattern_Analysis_", "").replace(".xlsx", "")
        
        # RAG sheet names
        rag_sheets = [
            'Config Patterns',
            'Matched Patterns',
            'Unseen Patterns',
            'Suggestions',
            'Summary'
        ]
        
        # Load consolidated file (NOT read-only)
        consolidated_wb = load_workbook(consolidated_file)
        
        # Load RAG file (read-only is OK here)
        rag_wb = load_workbook(latest_rag, read_only=True)
        
        sheets_processed = 0
        sheets_skipped = 0
        
        try:
            for sheet_name in rag_sheets:
                if sheet_name not in rag_wb.sheetnames:
                    logger.debug(f"  Sheet '{sheet_name}' not in RAG - skipping")
                    sheets_skipped += 1
                    continue
                
                rag_sheet = rag_wb[sheet_name]
                
                # ===== FIX 1: Check if sheet already exists =====
                if sheet_name in consolidated_wb.sheetnames:
                    consolidated_sheet = consolidated_wb[sheet_name]
                    logger.info(f"  {sheet_name}: Sheet exists (appending)")
                else:
                    # Create sheet if doesn't exist
                    consolidated_sheet = consolidated_wb.create_sheet(sheet_name)
                    logger.info(f"  {sheet_name}: Sheet created (new)")
                
                # Read data from RAG sheet
                data = []
                headers = []
                
                for idx, row in enumerate(rag_sheet.iter_rows(values_only=True)):
                    if idx == 0:
                        headers = list(row)
                    else:
                        if any(cell is not None and str(cell).strip() for cell in row):
                            data.append(list(row))
                
                # Skip if no data
                if not data:
                    logger.info(f"    → No data - SKIPPED")
                    sheets_skipped += 1
                    continue
                
                # ===== FIX 2: Check if sheet has headers =====
                current_max_row = consolidated_sheet.max_row
                
                # If sheet is empty (only default empty row), write headers
                if current_max_row == 1:
                    # Check if row 1 has any data
                    has_header = False
                    for cell in consolidated_sheet[1]:
                        if cell.value is not None:
                            has_header = True
                            break
                    
                    if not has_header:
                        # Write headers
                        for col_idx, header in enumerate(headers, 1):
                            consolidated_sheet.cell(row=1, column=col_idx, value=header)
                        start_row = 2
                        logger.info(f"    → Headers written to row 1")
                    else:
                        # Headers already exist
                        start_row = 2
                else:
                    # Sheet has data, append after last row
                    start_row = current_max_row + 1
                
                # ===== FIX 3: Create dataframe and append =====
                df = pd.DataFrame(data, columns=headers)
                
                # Add metadata
                if 'Source File' not in df.columns:
                    df['Source File'] = rag_filename
                if 'Timestamp' not in df.columns:
                    df['Timestamp'] = timestamp
                
                # Append rows
                for row_idx, row_data in enumerate(dataframe_to_rows(df, index=False, header=False)):
                    for col_idx, value in enumerate(row_data, 1):
                        consolidated_sheet.cell(row=start_row + row_idx, column=col_idx, value=value)
                
                logger.info(f"    ✓ Appended {len(data)} rows (rows {start_row}-{start_row + len(data) - 1})")
                sheets_processed += 1
            
            # ===== FIX 4: Save PROPERLY =====
            try:
                consolidated_wb.save(consolidated_file)
                logger.info(f"✓ File saved successfully: {consolidated_file}")
            except Exception as save_error:
                logger.error(f"Save error: {save_error}")
                # Try to close and reopen
                consolidated_wb.close()
                consolidated_wb = load_workbook(consolidated_file)
                consolidated_wb.save(consolidated_file)
                logger.info(f"✓ File saved (retry successful)")
            
            logger.info(f"✓ RAG consolidation: {sheets_processed} processed, {sheets_skipped} skipped")
        
        finally:
            try:
                consolidated_wb.close()
                rag_wb.close()
            except:
                pass
    
    except Exception as e:
        logger.error(f"RAG consolidation error: {e}", exc_info=True)
        raise
