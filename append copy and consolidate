def _append_rag_to_consolidated(self, consolidated_file: str):
    """
    Append RAG report data to consolidated file (Sheet2+)
    FIX: Properly handle existing sheets and append without recreating
    """
    try:
        # Find latest RAG report in temp_path
        rag_files = glob.glob(f"{self.temp_path}/RAG_Pattern_Analysis_*.xlsx")
        
        if not rag_files:
            logger.info("No RAG report found - skipping RAG consolidation")
            return
        
        # Get most recent RAG file
        latest_rag = max(rag_files, key=os.path.getctime)
        logger.info(f"Found RAG report: {os.path.basename(latest_rag)}")
        
        # Extract timestamp from filename
        rag_filename = os.path.basename(latest_rag)
        timestamp = rag_filename.replace("RAG_Pattern_Analysis_", "").replace(".xlsx", "")
        
        # RAG sheet names
        rag_sheets = [
            'Config Patterns',
            'Matched Patterns',
            'Unseen Patterns',
            'Suggestions',
            'Summary'
        ]
        
        # Load consolidated file (NOT read-only)
        consolidated_wb = load_workbook(consolidated_file)
        
        # Load RAG file (read-only is OK here)
        rag_wb = load_workbook(latest_rag, read_only=True)
        
        sheets_processed = 0
        sheets_skipped = 0
        
        try:
            for sheet_name in rag_sheets:
                if sheet_name not in rag_wb.sheetnames:
                    logger.debug(f"  Sheet '{sheet_name}' not in RAG - skipping")
                    sheets_skipped += 1
                    continue
                
                rag_sheet = rag_wb[sheet_name]
                
                # ===== FIX 1: Check if sheet already exists =====
                if sheet_name in consolidated_wb.sheetnames:
                    consolidated_sheet = consolidated_wb[sheet_name]
                    logger.info(f"  {sheet_name}: Sheet exists (appending)")
                else:
                    # Create sheet if doesn't exist
                    consolidated_sheet = consolidated_wb.create_sheet(sheet_name)
                    logger.info(f"  {sheet_name}: Sheet created (new)")
                
                # Read data from RAG sheet
                data = []
                headers = []
                
                for idx, row in enumerate(rag_sheet.iter_rows(values_only=True)):
                    if idx == 0:
                        headers = list(row)
                    else:
                        if any(cell is not None and str(cell).strip() for cell in row):
                            data.append(list(row))
                
                # Skip if no data
                if not 
                    logger.info(f"    → No data - SKIPPED")
                    sheets_skipped += 1
                    continue
                
                # ===== FIX 2: Check if sheet has headers =====
                current_max_row = consolidated_sheet.max_row
                
                # If sheet is empty (only default empty row), write headers
                if current_max_row == 1:
                    # Check if row 1 has any data
                    has_header = False
                    for cell in consolidated_sheet[1]:
                        if cell.value is not None:
                            has_header = True
                            break
                    
                    if not has_header:
                        # Write headers
                        for col_idx, header in enumerate(headers, 1):
                            consolidated_sheet.cell(row=1, column=col_idx, value=header)
                        start_row = 2
                        logger.info(f"    → Headers written to row 1")
                    else:
                        # Headers already exist
                        start_row = 2
                else:
                    # Sheet has data, append after last row
                    start_row = current_max_row + 1
                
                # ===== FIX 3: Create dataframe and append =====
                df = pd.DataFrame(data, columns=headers)
                
                # Add metadata
                if 'Source File' not in df.columns:
                    df['Source File'] = rag_filename
                if 'Timestamp' not in df.columns:
                    df['Timestamp'] = timestamp
                
                # Append rows
                for row_idx, row_data in enumerate(dataframe_to_rows(df, index=False, header=False)):
                    for col_idx, value in enumerate(row_data, 1):
                        consolidated_sheet.cell(row=start_row + row_idx, column=col_idx, value=value)
                
                logger.info(f"    ✓ Appended {len(data)} rows (rows {start_row}-{start_row + len(data) - 1})")
                sheets_processed += 1
            
            # ===== FIX 4: Save PROPERLY =====
            try:
                consolidated_wb.save(consolidated_file)
                logger.info(f"✓ File saved successfully: {consolidated_file}")
            except Exception as save_error:
                logger.error(f"Save error: {save_error}")
                # Try to close and reopen
                consolidated_wb.close()
                consolidated_wb = load_workbook(consolidated_file)
                consolidated_wb.save(consolidated_file)
                logger.info(f"✓ File saved (retry successful)")
            
            logger.info(f"✓ RAG consolidation: {sheets_processed} processed, {sheets_skipped} skipped")
        
        finally:
            try:
                consolidated_wb.close()
                rag_wb.close()
            except:
                pass
    
    except Exception as e:
        logger.error(f"RAG consolidation error: {e}", exc_info=True)
        raise












"""
COMPLETE UPDATED CODE
- Integrates RAG consolidation with existing export_consolidated_results
- Appends RAG data from Sheet2 onwards
- Keeps Sheet1 for your existing trade data
"""

import os
import glob
import shutil
import logging
from datetime import datetime
import pandas as pd
from pathlib import Path
from openpyxl import load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from filelock import FileLock
import csv

logger = logging.getLogger(__name__)


# ============================================================================
# EXISTING: export_consolidated_results (UPDATED with RAG integration)
# ============================================================================

def export_consolidated_results(self):
    """
    Export consolidated results by appending all unique temp trade CSVs
    to a single Excel file, robust for multiple simultaneous app runs.
    
    UPDATED: Also appends RAG data to Sheet2+ if RAG report exists
    """
    try:
        temp_df = None
        today = datetime.now()
        today_str = today.strftime("%m_%d_%y")
        csv_files = glob.glob(f"{self.temp_path}/*.csv")
        sheet_name = "Consolidated_Report"
        consolidated_file = f"{self.out_path}/{today_str}_{sheet_name}.xlsx"
        lock_path = consolidated_file + '.lock'
        ws_rows = []
        
        # Use a lock so only one process writes at a time
        with FileLock(lock_path, timeout=60):
            for file in csv_files:
                base_name = os.path.basename(file)
                trade_id = base_name.split("_")[0]
                rows_found = False
                
                with open(file, newline="", encoding="utf-8") as f:
                    reader = csv.DictReader(f)
                    csv_headers = list(reader.fieldnames) if reader.fieldnames else []
                    expected_headers = self.static_headers[1:]
                    
                    # Headers from index 16 onward in self.static_headers
                    required_sub_headers = self.static_headers[19:]
                    
                    # Check if required sub headers exist in csv headers
                    if all(header in csv_headers for header in required_sub_headers):
                        # Append data rows as normal if all required headers exist
                        for row in reader:
                            out_row = [trade_id]
                            for header in expected_headers:
                                out_row.append(row.get(header, ""))
                            ws_rows.append(out_row)
                            rows_found = True
                    
                    elif any(header in csv_headers for header in expected_headers[1:]):
                        # Append data rows as normal if all required headers exist
                        for row in reader:
                            out_row = [trade_id]
                            for header in expected_headers:
                                out_row.append(row.get(header, ""))
                            ws_rows.append(out_row)
                            rows_found = True
                    
                    else:
                        if len(csv_headers) == 0:
                            out_row = [f"{trade_id} - Found in Mail - No Entity Found"] + [""] * (len(self.static_headers) - 1)
                        else:
                            out_row = [f"{trade_id} - Partial header mismatch - Check file: {base_name}"] + [""] * (len(self.static_headers) - 1)
                        ws_rows.append(out_row)
                        rows_found = True
                    
                    f.close()
                
                if not rows_found:
                    out_row = [f"{trade_id} - Not Found in Mail"] + [""] * (len(self.static_headers) - 1)
                    ws_rows.append(out_row)
                    os.remove(file)
            
            # Build dataframe for all combined results
            df_new = pd.DataFrame(ws_rows, columns=self.static_headers)
            
            # Append to existing or create new Excel
            if os.path.isfile(consolidated_file):
                df_existing = pd.read_excel(consolidated_file)
                df_combined = pd.concat([df_existing, df_new], ignore_index=True)
                df_combined.to_excel(consolidated_file, index=False)
            else:
                df_new.to_excel(consolidated_file, index=False)
            
            # Optionally display summary, e.g. in Tkinter widget
            temp_df = pd.read_excel(consolidated_file)
            if not temp_df.empty:
                self.entity_result_text.delete(1.0, tk.END)
                self.entity_result_text.insert(tk.END, f"{temp_df}\n")
            else:
                self.entity_result_text.insert(tk.END, "No Data found in this email")
        
        logger.info("Success - Consolidated Report File Generated")
        
        # ===== NEW: APPEND RAG DATA TO SHEET2+ =====
        self._append_rag_to_consolidated(consolidated_file)
        
        # Copy files and cleanup
        self.copy_files(self.temp_path, self.out_path)
        
        # Get shared path from config
        shared_path = self.out_path  # Or use your shared path
        
        messagebox.showinfo("Success", f"Consolidated Report File Generated: {self.out_path}")
    
    except Exception as e:
        logger.info(f"Export Error Failed to export : {e}")
        messagebox.showerror("Export Error", f"Failed to export : {e}")


# ============================================================================
# NEW: _append_rag_to_consolidated
# ============================================================================

def _append_rag_to_consolidated(self, consolidated_file: str):
    """
    Append RAG report data to consolidated file (Sheet2+)
    Called after trade data is written to Sheet1
    """
    try:
        # Find latest RAG report in temp_path
        rag_files = glob.glob(f"{self.temp_path}/RAG_Pattern_Analysis_*.xlsx")
        
        if not rag_files:
            logger.info("No RAG report found - skipping RAG consolidation")
            return
        
        # Get most recent RAG file
        latest_rag = max(rag_files, key=os.path.getctime)
        logger.info(f"Found RAG report: {os.path.basename(latest_rag)}")
        
        # Extract timestamp from filename
        rag_filename = os.path.basename(latest_rag)
        timestamp = rag_filename.replace("RAG_Pattern_Analysis_", "").replace(".xlsx", "")
        
        # RAG sheet names
        rag_sheets = [
            'Config Patterns',
            'Matched Patterns',
            'Unseen Patterns',
            'Suggestions',
            'Summary'
        ]
        
        # Load consolidated file
        consolidated_wb = load_workbook(consolidated_file)
        
        # Load RAG file
        rag_wb = load_workbook(latest_rag, read_only=True)
        
        sheets_processed = 0
        sheets_skipped = 0
        
        try:
            for sheet_name in rag_sheets:
                if sheet_name not in rag_wb.sheetnames:
                    logger.debug(f"  Sheet '{sheet_name}' not in RAG - skipping")
                    sheets_skipped += 1
                    continue
                
                # Create sheet in consolidated if doesn't exist
                if sheet_name not in consolidated_wb.sheetnames:
                    consolidated_wb.create_sheet(sheet_name)
                    logger.info(f"  Created sheet: {sheet_name}")
                
                rag_sheet = rag_wb[sheet_name]
                consolidated_sheet = consolidated_wb[sheet_name]
                
                # Read data from RAG sheet
                data = []
                headers = []
                
                for idx, row in enumerate(rag_sheet.iter_rows(values_only=True)):
                    if idx == 0:
                        headers = list(row)
                    else:
                        if any(cell is not None and str(cell).strip() for cell in row):
                            data.append(list(row))
                
                # Skip if no data
                if not 
                    logger.info(f"  {sheet_name}: No data - SKIPPED")
                    sheets_skipped += 1
                    continue
                
                # Create dataframe
                df = pd.DataFrame(data, columns=headers)
                
                # Add metadata
                if 'Source File' not in df.columns:
                    df['Source File'] = rag_filename
                if 'Timestamp' not in df.columns:
                    df['Timestamp'] = timestamp
                
                # If sheet is empty (first time), write headers
                if consolidated_sheet.max_row == 1:
                    # Write headers
                    for col_idx, header in enumerate(df.columns, 1):
                        consolidated_sheet.cell(row=1, column=col_idx, value=header)
                    start_row = 2
                else:
                    start_row = consolidated_sheet.max_row + 1
                
                # Append data
                for row_idx, row_data in enumerate(dataframe_to_rows(df, index=False, header=False)):
                    for col_idx, value in enumerate(row_data, 1):
                        consolidated_sheet.cell(row=start_row + row_idx, column=col_idx, value=value)
                
                logger.info(f"  ✓ {sheet_name}: Appended {len(data)} rows")
                sheets_processed += 1
            
            # Save consolidated file
            consolidated_wb.save(consolidated_file)
            logger.info(f"✓ RAG data appended: {sheets_processed} sheets processed, {sheets_skipped} skipped")
        
        finally:
            consolidated_wb.close()
            rag_wb.close()
    
    except Exception as e:
        logger.error(f"RAG consolidation error: {e}", exc_info=True)


# ============================================================================
# UPDATED: extract_entities - Generate RAG report in temp_path
# ============================================================================

def extract_entities(self):
    """Extract entities with RAG report generation"""
    
    # ... [Your existing extraction code] ...
    
    try:
        # ... [extraction logic] ...
        
        # Generate RAG report IN TEMP PATH
        print(f"\n[2/2] Generating RAG report...")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ===== SAVE RAG REPORT TO TEMP PATH =====
        rag_report_path = f"{self.temp_path}/RAG_Pattern_Analysis_{timestamp}.xlsx"
        
        rag_results = self.rag_extractor.finish_and_generate_report(
            rag_report_path,
            matched_data=matched_patterns
        )
        
        print(f"✓ RAG report: {rag_report_path}")
        print(f"  Matched: {matched_count}")
        print(f"  Unseen: {unseen_count}")
        
        messagebox.showinfo("Success", 
            f"Extraction complete!\n\n"
            f"Matched: {matched_count}\n"
            f"Unseen: {unseen_count}\n\n"
            f"RAG report will be consolidated when you export")
    
    except Exception as e:
        logger.error(f"Error: {str(e)}", exc_info=True)
        messagebox.showerror("Error", f"Failed: {str(e)}")


# ============================================================================
# OPTIONAL: Clean up RAG files after consolidation
# ============================================================================

def copy_files(self, source_path, destination_path):
    """Copy files from temp to output (UPDATED to cleanup RAG files)"""
    try:
        for file in os.listdir(source_path):
            file_path = os.path.join(source_path, file)
            if os.path.isfile(file_path):
                # Skip RAG reports (already consolidated)
                if file.startswith("RAG_Pattern_Analysis_"):
                    logger.info(f"Skipping RAG file (already consolidated): {file}")
                    os.remove(file_path)  # Delete after consolidation
                    continue
                
                # Copy other files
                shutil.copy2(file_path, destination_path)
        
        logger.info(f"Files copied from {source_path} to {destination_path}")
    
    except Exception as e:
        logger.error(f"File copy error: {e}")
