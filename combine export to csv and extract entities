def extract_and_export(self):
    """
    COMPLETE & OPTIMIZED: Extract entities with conversation-aware activity tracking
    - Detects email conversation boundaries (replies, forwards)
    - Resets activity for each conversation in thread
    - Package-scoped activity within each conversation
    - Pre-compiled patterns (no repeated compilation)
    - Cached attribute checking
    - Table enrichment with Cust_ID matching
    """
    
    if self.selected_email is None:
        messagebox.showwarning("No Selection", "Please select an email from the chain first")
        return
    
    try:
        # Get entity definitions
        entity_json = self.entity_text.get(1.0, tk.END).strip()
        self.entity_definitions = json.loads(entity_json)
    except json.JSONDecodeError as e:
        messagebox.showerror("JSON Error", f"Invalid JSON: {str(e)}")
        return
    
    try:
        start_time = time.time()
        
        # Stop previous RAG
        if hasattr(self, 'rag_extractor') and self.rag_extractor:
            logger.info("Stopping previous RAG processor...")
            self.rag_extractor.rag_processor.stop_and_wait(timeout=1.0)
            self.rag_extractor = None
        
        # Create fresh RAG processor
        logger.info("Creating new RAG processor...")
        self.rag_extractor = RAGEnabledExtractor(self.pattern_manager, self.usage_tracker)
        
        entity_names = self.pattern_manager.get_all_entity_names()
        email_data = self.selected_email
        
        # ===== OPTIMIZATION: Pre-compile all patterns ONCE =====
        compiled_patterns_map = {}
        
        for entity_name in entity_names:
            patterns = self.pattern_manager.get_patterns_for_entity(entity_name)
            compiled_list = []
            
            for pattern_entry in patterns:
                if pattern_entry.get('entity_type') == 'pattern':
                    pattern_str = pattern_entry['pattern']
                    pattern_id = pattern_entry['pattern_id']
                    
                    try:
                        compiled_regex = re.compile(pattern_str, re.IGNORECASE)
                        compiled_list.append({
                            'pattern_id': pattern_id,
                            'pattern_str': pattern_str,
                            'compiled_regex': compiled_regex,
                            'entity_name': entity_name
                        })
                    except re.error as e:
                        logger.error(f"Regex compilation error for {pattern_id}: {e}")
            
            compiled_patterns_map[entity_name] = compiled_list
        
        logger.info(f"‚è± Pattern compilation: {time.time() - start_time:.2f}s")
        
        # ===== Initialize data structures =====
        extraction_records = []
        matched_patterns = []
        all_tables = []
        
        # ===== Prepare email lines =====
        email_lines = email_data['body'].splitlines()
        email_lines = self.email_cleaner.clean_email_body(email_lines)
        email_lines = [line.strip() for line in email_lines if line.strip()]
        
        matched_count = 0
        unseen_count = 0
        
        extraction_start = time.time()
        
        # ===== CONVERSATION BOUNDARY DETECTION =====
        def is_conversation_boundary(line, line_idx, email_lines):
            """
            Detect if this line starts a new email conversation (reply/forward)
            Returns True if this is a conversation separator
            """
            line_lower = line.lower()
            
            # Common email reply/forward patterns
            reply_patterns = [
                'from:', 'sent:', 'to:', 'subject:',  # Email headers
                'on ', ' wrote:',  # "On [date], [person] wrote:"
                '-----original message-----',
                '--- forwarded message ---',
                '________________________________',  # Outlook separator
                '==================',
                'begin forwarded message',
                're:', 'fw:', 'fwd:',  # Subject line indicators
            ]
            
            # Check for reply/forward indicators
            for pattern in reply_patterns:
                if pattern in line_lower:
                    # Additional validation: check if next few lines also look like headers
                    if 'from:' in line_lower or 'sent:' in line_lower:
                        return True
                    
                    # Check if this is part of email header block
                    if line_idx + 1 < len(email_lines):
                        next_line = email_lines[line_idx + 1].lower()
                        if any(p in next_line for p in ['from:', 'to:', 'sent:', 'subject:']):
                            return True
            
            # Check for timestamp patterns (e.g., "On Mon, Nov 18, 2025 at 3:00 PM")
            import re
            timestamp_pattern = r'on\s+\w+,?\s+\w+\s+\d+,?\s+\d{4}'
            if re.search(timestamp_pattern, line_lower):
                return True
            
            return False
        
        # ===== PACKAGE BOUNDARY DETECTION =====
        def is_package_boundary(line, prev_line):
            """
            Detect if this line starts a new trade package
            Returns True if this is likely a new package
            """
            line_upper = line.upper()
            
            # Indicators of new package:
            # 1. Empty line or separator
            if not line.strip() or line.strip() in ['---', '===', '***', '___']:
                return True
            
            # 2. New activity keyword on standalone line
            for keyword in self.activity_keywords:
                if keyword in line_upper and len(line_upper.split()) < 5:
                    return True
            
            # 3. Repeating header patterns
            package_headers = ['TRADE DETAILS', 'PACKAGE', 'DEAL #', 'TRADE #', 
                             'TRANSACTION', 'BOOKING', 'CONFIRMATION']
            for header in package_headers:
                if header in line_upper and len(line_upper) < 30:
                    return True
            
            return False
        
        # ===== CONVERSATION & PACKAGE-AWARE EXTRACTION =====
        current_activity = None
        current_package_id = 0
        current_conversation_id = 0  # Track conversation threads
        
        for line_idx, line in enumerate(email_lines):
            prev_line = email_lines[line_idx - 1] if line_idx > 0 else ""
            
            # ===== CHECK FOR CONVERSATION BOUNDARY (Higher Priority) =====
            if is_conversation_boundary(line, line_idx, email_lines):
                current_conversation_id += 1
                current_package_id = 0  # Reset package counter for new conversation
                current_activity = None  # ‚úÖ RESET ACTIVITY FOR NEW CONVERSATION
                logger.info(f"üìß New conversation #{current_conversation_id} detected at line {line_idx}: {line[:50]}")
                continue  # Skip processing this line (it's a header)
            
            # ===== CHECK FOR PACKAGE BOUNDARY (Within Conversation) =====
            if is_package_boundary(line, prev_line):
                current_package_id += 1
                current_activity = None  # Reset activity for new package
                logger.info(f"üì¶ New package #{current_package_id} in conversation #{current_conversation_id} at line {line_idx}")
            
            # ===== CHECK FOR ACTIVITY KEYWORD =====
            line_upper = line.upper()
            for keyword in self.activity_keywords:
                if keyword in line_upper and len(line_upper.split()) < 5:
                    current_activity = line_upper
                    logger.info(f"üè∑Ô∏è Activity '{current_activity}' set for conversation #{current_conversation_id}, package #{current_package_id}")
                    break
            
            extracted_from_line = False
            
            # ===== PATTERN EXTRACTION =====
            for entity_name, compiled_patterns in compiled_patterns_map.items():
                if extracted_from_line:
                    break
                
                for pattern_info in compiled_patterns:
                    compiled_regex = pattern_info['compiled_regex']
                    pattern_str = pattern_info['pattern_str']
                    pattern_id = pattern_info['pattern_id']
                    
                    # Use optimized attribute check
                    if not self.attribute_count_check(pattern_str, line):
                        continue
                    
                    # Try to match with pre-compiled regex
                    matches = compiled_regex.finditer(line)
                    
                    for match in matches:
                        groupdict = match.groupdict()
                        
                        if not groupdict:
                            continue
                        
                        # ===== CREATE RECORD WITH CONVERSATION & PACKAGE INFO =====
                        record = {
                            'Trade_ID': email_data['trade_id'],
                            'Email_Date': email_data.get('date', ''),
                            'Email_From': email_data.get('sender', ''),
                            'Email_Subject': email_data.get('subject', ''),
                            'Conversation_ID': current_conversation_id,  # Track conversation
                            'Package_ID': current_package_id  # Track package within conversation
                        }
                        
                        # Add activity (conversation & package scoped)
                        if current_activity:
                            record['ActivityType'] = current_activity
                        
                        # Add extracted entities
                        extracted_labels = []
                        for k, v in groupdict.items():
                            if v:
                                record[k] = v
                                extracted_labels.append(k)
                        
                        if extracted_labels:
                            # Add metadata
                            record['Source_Line'] = line
                            record['Pattern_ID'] = pattern_id
                            record['Line_Index'] = line_idx
                            
                            # Add to records
                            extraction_records.append(record)
                            
                            # Track for RAG
                            matched_patterns.append({
                                'line': line,
                                'pattern_id': pattern_id,
                                'entity_name': entity_name,
                                'extracted_labels': extracted_labels,
                                'confidence': f"{len(extracted_labels)}/{len(groupdict)}",
                                'activity': current_activity,
                                'conversation_id': current_conversation_id,
                                'package_id': current_package_id
                            })
                            
                            self.usage_tracker.record_match(pattern_id, line)
                            extracted_from_line = True
                            matched_count += 1
                            break
                    
                    if extracted_from_line:
                        break
            
            # ===== GAZETTEER EXTRACTION =====
            if not extracted_from_line:
                for entity_name in entity_names:
                    entity_def = self.entity_definitions.get(entity_name, {})
                    if entity_def.get('entity_type') == 'gazetteer':
                        gazetteer_vals = entity_def.get("values", [])
                        subject_vals = self.extract_with_gazetteer(email_data['subject'], gazetteer_vals)
                        body_vals = self.extract_with_gazetteer(line, gazetteer_vals)
                        
                        all_vals = subject_vals + body_vals
                        
                        if all_vals:
                            record = {
                                'Trade_ID': email_data['trade_id'],
                                'Email_Date': email_data.get('date', ''),
                                'Email_From': email_data.get('sender', ''),
                                'Email_Subject': email_data.get('subject', ''),
                                'Conversation_ID': current_conversation_id,
                                'Package_ID': current_package_id,
                                entity_name: ', '.join(all_vals),
                                'Source_Line': line,
                                'Pattern_ID': 'GAZETTEER',
                                'Line_Index': line_idx
                            }
                            
                            # Add activity
                            if current_activity:
                                record['ActivityType'] = current_activity
                            
                            extraction_records.append(record)
                            
                            matched_patterns.append({
                                'line': line,
                                'pattern_id': 'GAZETTEER',
                                'entity_name': entity_name,
                                'extracted_labels': all_vals,
                                'confidence': 'N/A',
                                'activity': current_activity,
                                'conversation_id': current_conversation_id,
                                'package_id': current_package_id
                            })
                            
                            extracted_from_line = True
                            matched_count += 1
                            break
            
            # Queue unseen
            if not extracted_from_line:
                unseen_count += 1
                self.rag_extractor.queue_unseen(line, "Trade")
        
        logger.info(f"‚è± Pattern extraction: {time.time() - extraction_start:.2f}s")
        logger.info(f"üìß Total conversations detected: {current_conversation_id}")
        logger.info(f"üì¶ Total packages detected: {current_package_id}")
        
        # ===== CREATE entity_df FROM EXTRACTION RECORDS =====
        if extraction_records:
            entity_df = pd.DataFrame(extraction_records)
        else:
            entity_df = pd.DataFrame()
        
        # ===== REST OF CODE (Table extraction, enrichment, CSV export) =====
        # [Same as before - table extraction, enrichment, CSV export, RAG report]
        
        # ===== FAST TABLE EXTRACTION =====
        table_start = time.time()
        html_content = email_data.get('html_body', '')
        table_df = None
        
        if html_content:
            try:
                soup = BeautifulSoup(html_content, 'html.parser')
                tables = soup.find_all('table')
                
                table_records = []
                
                for table_idx, table in enumerate(tables, 1):
                    headers = []
                    header_row = table.find('tr')
                    if header_row:
                        headers = [th.get_text(strip=True) for th in header_row.find_all('th')]
                    
                    rows = [[col.get_text(strip=True) for col in tr.find_all(['td', 'th'])] 
                            for tr in table.find_all('tr')[1:]]
                    
                    all_tables.append({
                        'Email_Index': 1,
                        'Headers': headers,
                        'rows': rows
                    })
                    
                    for row in rows:
                        row_filtered = list(filter(None, row))
                        cleaned = [item for item in row_filtered if item not in [':', '-', '']]
                        
                        if len(cleaned) >= 2:
                            key = cleaned[0]
                            value = " ".join(cleaned[1:])
                            
                            table_record = {
                                key: value,
                                'Trade_ID': email_data['trade_id'],
                                'Email_Date': email_data.get('date', ''),
                                'Email_From': email_data.get('sender', ''),
                                'Email_Subject': email_data.get('subject', ''),
                                'Source': f'Table_{table_idx}',
                                'Pattern_ID': 'TABLE'
                            }
                            table_records.append(table_record)
                
                if table_records:
                    table_df = pd.DataFrame(table_records)
                    table_df = table_df.dropna(how='all')
                    table_df = table_df.dropna(how='all', axis=1)
                    table_df = table_df.agg(lambda x: x.dropna().iloc[0] if x.notna().any() else np.nan).to_frame().T
            
            except Exception as e:
                logger.error(f"Table extraction error: {e}")
        
        logger.info(f"‚è± Table extraction: {time.time() - table_start:.2f}s")
        
        # ===== TABLE ENRICHMENT =====
        enrichment_start = time.time()
        enriched_records = []
        
        if not entity_df.empty and table_df is not None and not table_df.empty:
            if 'Cust_ID' in entity_df.columns and 'Cust_ID' in table_df.columns:
                first_row_df2 = table_df.iloc[0].to_dict()
                
                for _, row in entity_df.iterrows():
                    row_dict = row.to_dict()
                    
                    cust_id_entity = str(row_dict.get("Cust_ID", ""))
                    cust_id_table = str(first_row_df2.get("Cust_ID", ""))
                    
                    if cust_id_entity and cust_id_table != "NONE" and cust_id_entity in cust_id_table:
                        row_dict.update(first_row_df2)
                    
                    enriched_records.append(row_dict)
                
                final_df = pd.DataFrame(enriched_records)
            else:
                final_df = pd.concat([entity_df, table_df], ignore_index=True)
        elif not entity_df.empty:
            final_df = entity_df.copy()
        elif table_df is not None and not table_df.empty:
            final_df = table_df.copy()
        else:
            final_df = pd.DataFrame(columns=['Trade_ID', 'Email_Date', 'Email_From', 'Email_Subject'])
        
        logger.info(f"‚è± Table enrichment: {time.time() - enrichment_start:.2f}s")
        
        # ===== REORDER COLUMNS =====
        if not final_df.empty:
            metadata_cols = ['Trade_ID', 'Email_Date', 'Email_From', 'Email_Subject', 
                           'Conversation_ID', 'Package_ID', 'ActivityType', 
                           'Pattern_ID', 'Source', 'Source_Line', 'Line_Index']
            
            entity_cols = [col for col in final_df.columns if col not in metadata_cols]
            existing_metadata = [col for col in metadata_cols if col in final_df.columns]
            column_order = existing_metadata + sorted(entity_cols)
            
            final_df = final_df[column_order]
        
        # ===== EXPORT TO CSV =====
        csv_start = time.time()
        today_str = datetime.now().strftime("%m_%d_%y")
        filename = f"{email_data['trade_id']}_email_entities_export_{today_str}.csv"
        out_file = os.path.join(self.temp_path, filename)
        
        if os.path.exists(out_file):
            existing = pd.read_csv(out_file)
            all_cols = list(set(existing.columns).union(set(final_df.columns)))
            existing = existing.reindex(columns=all_cols)
            final_df = final_df.reindex(columns=all_cols)
            combined = pd.concat([existing, final_df], ignore_index=True)
            combined = combined.drop_duplicates().reset_index(drop=True)
            combined.to_csv(out_file, index=False)
        else:
            final_df.to_csv(out_file, index=False)
        
        logger.info(f"‚è± CSV export: {time.time() - csv_start:.2f}s")
        
        # ===== UI UPDATE =====
        self._update_ui_display_with_conversations(extraction_records, matched_count, 
                                                   unseen_count, len(all_tables), 
                                                   current_conversation_id)
        
        # ===== RAG REPORT =====
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        rag_report_path = f"{self.temp_path}/RAG_Pattern_Analysis_{timestamp}.xlsx"
        self.rag_extractor.finish_and_generate_report(rag_report_path, matched_data=matched_patterns)
        
        # ===== FINAL SUMMARY =====
        total_time = time.time() - start_time
        
        messagebox.showinfo("Success", 
            f"Extraction Complete!\n\n"
            f"‚úì Records: {len(final_df)}\n"
            f"‚úì Pattern Matches: {matched_count}\n"
            f"‚ö† Unseen: {unseen_count}\n"
            f"üìß Conversations: {current_conversation_id}\n"
            f"üì¶ Packages: {current_package_id}\n"
            f"üìã Tables: {len(all_tables)}\n"
            f"‚è± Time: {total_time:.2f}s\n\n"
            f"CSV: {filename}")
    
    except Exception as e:
        logger.error(f"Error: {str(e)}", exc_info=True)
        messagebox.showerror("Error", f"Failed: {str(e)}")


# ============================================================================
# UPDATED UI DISPLAY WITH CONVERSATION INFO
# ============================================================================

def _update_ui_display_with_conversations(self, extraction_records, matched_count, 
                                         unseen_count, table_count, conversation_count):
    """Update UI with conversation and package information"""
    output_lines = []
    
    output_lines.append("=" * 70)
    output_lines.append("EXTRACTED ENTITIES (Conversation-Aware View)")
    output_lines.append("=" * 70)
    output_lines.append("")
    
    if extraction_records:
        from collections import defaultdict
        
        # Group by conversation
        conversations = defaultdict(lambda: defaultdict(list))
        for record in extraction_records:
            conv_id = record.get('Conversation_ID', 0)
            pkg_id = record.get('Package_ID', 0)
            conversations[conv_id][pkg_id].append(record)
        
        output_lines.append(f"üìß Total Conversations: {conversation_count}")
        output_lines.append("")
        
        # Show first 2 conversations
        for conv_id in sorted(conversations.keys())[:2]:
            packages = conversations[conv_id]
            total_records = sum(len(records) for records in packages.values())
            
            output_lines.append(f"üìß Conversation #{conv_id} ({total_records} records, {len(packages)} packages):")
            
            # Show first 2 packages in conversation
            for pkg_id in sorted(packages.keys())[:2]:
                pkg_records = packages[pkg_id]
                output_lines.append(f"  üì¶ Package #{pkg_id} ({len(pkg_records)} records):")
                
                activity = pkg_records[0].get('ActivityType') if pkg_records else None
                if activity:
                    output_lines.append(f"     üè∑Ô∏è Activity: {activity}")
                
                # Show first record
                if pkg_records:
                    record = pkg_records[0]
                    output_lines.append(f"     Sample:")
                    for key, val in record.items():
                        if key not in ['Trade_ID', 'Email_Date', 'Email_From', 'Email_Subject',
                                      'Conversation_ID', 'Package_ID', 'Pattern_ID', 
                                      'Source_Line', 'Line_Index'] and val:
                            output_lines.append(f"       {key}: {val}")
                
                if len(pkg_records) > 1:
                    output_lines.append(f"     ... and {len(pkg_records) - 1} more records")
            
            if len(packages) > 2:
                output_lines.append(f"  ... and {len(packages) - 2} more packages")
            output_lines.append("")
        
        if len(conversations) > 2:
            output_lines.append(f"... and {len(conversations) - 2} more conversations")
            output_lines.append("")
    
    output_lines.append("-" * 70)
    output_lines.append("EXTRACTION STATISTICS")
    output_lines.append("-" * 70)
    output_lines.append(f"‚úì Total records: {len(extraction_records)}")
    output_lines.append(f"‚úì Matched: {matched_count}")
    output_lines.append(f"‚ö† Unseen: {unseen_count}")
    output_lines.append(f"üìß Conversations: {conversation_count}")
    output_lines.append(f"üìã Tables: {table_count}")
    
    if matched_count + unseen_count > 0:
        match_rate = (matched_count / (matched_count + unseen_count)) * 100
        output_lines.append(f"üìà Match rate: {match_rate:.1f}%")
    
    self.content_text.delete(1.0, tk.END)
    self.content_text.insert(tk.END, "\n".join(output_lines))
