"""
OPTIMIZED: Single extraction flow
- One button click does: Extract + Export CSV + Display + RAG
- No duplicate processing
- Maximum performance
"""

# ============================================================================
# SINGLE UNIFIED EXTRACTION (Extract + Export in one pass)
# ============================================================================

def extract_and_export(self):
    """
    OPTIMIZED: Single pass extraction that:
    1. Extracts entities from email body
    2. Extracts tables from HTML
    3. Saves to CSV
    4. Displays results in UI
    5. Generates RAG report
    
    All in ONE iteration!
    """
    
    if self.selected_email is None:
        messagebox.showwarning("No Selection", "Please select an email from the chain first")
        return
    
    try:
        # Get entity definitions
        entity_json = self.entity_text.get(1.0, tk.END).strip()
        self.entity_definitions = json.loads(entity_json)
    except json.JSONDecodeError as e:
        messagebox.showerror("JSON Error", f"Invalid JSON: {str(e)}")
        return
    
    try:
        # Stop previous RAG
        if hasattr(self, 'rag_extractor') and self.rag_extractor:
            logger.info("Stopping previous RAG processor...")
            self.rag_extractor.rag_processor.stop_and_wait(timeout=1.0)
            self.rag_extractor = None
        
        # Create fresh RAG processor
        logger.info("Creating new RAG processor...")
        self.rag_extractor = RAGEnabledExtractor(self.pattern_manager, self.usage_tracker)
        
        entity_names = self.pattern_manager.get_all_entity_names()
        email_data = self.selected_email
        
        # Initialize collections
        entities = {}
        matched_patterns = []
        all_data = []
        all_tables = []
        
        # Prepare email lines
        email_lines = email_data['body'].splitlines()
        clean_body = self.email_cleaner.clean_email_body(email_lines)
        email_lines = list(filter(None, email_lines))
        
        current_activity = None
        matched_count = 0
        unseen_count = 0
        
        # ===== SINGLE PASS: Pattern Extraction =====
        for line in email_lines:
            line = line.strip()
            if not line:
                continue
            
            # Check for activity keywords
            line_upper = line.upper()
            for keyword in self.activity_keywords:
                if keyword in line_upper:
                    len_words = self.count_words(line_upper)
                    if len_words < 5:
                        current_activity = line_upper
                        logger.info(f"Activity type changed to: {current_activity}")
                        break
            
            extracted_from_line = False
            
            # Process each entity
            for entity_name in entity_names:
                patterns = self.pattern_manager.get_patterns_for_entity(entity_name)
                
                for pattern_entry in patterns:
                    entity_type = pattern_entry.get('entity_type', 'pattern')
                    
                    if entity_type == "pattern":
                        pattern_id = pattern_entry['pattern_id']
                        pattern_regex = pattern_entry['pattern']
                        
                        self.usage_tracker.record_attempt(pattern_id)
                        
                        try:
                            if self.attribute_count_check(pattern_regex, line):
                                regex = re.compile(pattern_regex, re.IGNORECASE)
                                matches = regex.finditer(line)
                                
                                for match in matches:
                                    groupdict = match.groupdict()
                                    
                                    if current_activity:
                                        groupdict['ActivityType'] = current_activity
                                    
                                    extracted_labels = []
                                    for k, v in groupdict.items():
                                        if v:
                                            entities.setdefault(k, []).append(f"{v}")
                                            extracted_labels.append(k)
                                    
                                    self.usage_tracker.record_match(pattern_id, line)
                                    extracted_from_line = True
                                    matched_count += 1
                                    
                                    matched_patterns.append({
                                        'line': line,
                                        'pattern_id': pattern_id,
                                        'entity_name': entity_name,
                                        'extracted_labels': extracted_labels,
                                        'confidence': f"{len(extracted_labels)}/{len(groupdict)}"
                                    })
                                    break
                        
                        except re.error as regex_error:
                            logger.error(f"Regex error {pattern_id}: {regex_error}")
                    
                    elif entity_type == "gazetteer":
                        entity_def = self.entity_definitions.get(entity_name, {})
                        gazetteer_vals = entity_def.get("values", [])
                        subject_vals = self.extract_with_gazetteer(email_data['subject'], gazetteer_vals)
                        body_vals = self.extract_with_gazetteer(line, gazetteer_vals)
                        
                        all_vals = subject_vals + body_vals
                        
                        if all_vals:
                            entities[entity_name] = all_vals
                            extracted_from_line = True
                            matched_count += 1
                            
                            matched_patterns.append({
                                'line': line,
                                'pattern_id': 'GAZETTEER',
                                'entity_name': entity_name,
                                'extracted_labels': all_vals,
                                'confidence': 'N/A'
                            })
            
            # Queue unseen
            if not extracted_from_line:
                unseen_count += 1
                self.rag_extractor.queue_unseen(line, "Trade")
        
        # ===== SINGLE PASS: Table Extraction =====
        html_content = email_data.get('html_body', '')
        if html_content:
            soup = BeautifulSoup(html_content, 'html.parser')
            tables = soup.find_all('table')
            
            for table in tables:
                headers = []
                rows = []
                
                # Extract headers
                header_row = table.find('tr')
                if header_row:
                    headers = [th.get_text(strip=True) for th in header_row.find_all('th')]
                
                # Extract rows
                for tr in table.find_all('tr')[1:]:
                    cols = tr.find_all(['td', 'th'])
                    row_data = [col.get_text(strip=True) for col in cols]
                    rows.append(row_data)
                
                # Store table data
                all_tables.append({
                    'Email_Index': 1,
                    'Date': email_data.get('date'),
                    'From': email_data.get('sender'),
                    'To': email_data.get('recipient'),
                    'Subject': email_data.get('subject', ''),
                    'Headers': headers,
                    'rows': rows
                })
        
        # ===== STEP 1: DISPLAY IN UI =====
        self.content_text.delete(1.0, tk.END)
        
        self.content_text.insert(tk.END, "=" * 70 + "\n")
        self.content_text.insert(tk.END, "EXTRACTED ENTITIES\n")
        self.content_text.insert(tk.END, "=" * 70 + "\n\n")
        
        if entities:
            for entity_name, vals in entities.items():
                self.content_text.insert(tk.END, f"üìå {entity_name}:\n")
                for v in vals:
                    self.content_text.insert(tk.END, f"   ‚Ä¢ {v}\n")
                self.content_text.insert(tk.END, "\n")
        else:
            self.content_text.insert(tk.END, "‚ö†Ô∏è  No entities found in this email\n\n")
        
        # Show tables if any
        if all_tables:
            self.content_text.insert(tk.END, "-" * 70 + "\n")
            self.content_text.insert(tk.END, f"EXTRACTED TABLES ({len(all_tables)} found)\n")
            self.content_text.insert(tk.END, "-" * 70 + "\n\n")
            
            for idx, table in enumerate(all_tables, 1):
                self.content_text.insert(tk.END, f"Table {idx}:\n")
                self.content_text.insert(tk.END, f"  Headers: {', '.join(table['Headers'])}\n")
                self.content_text.insert(tk.END, f"  Rows: {len(table['rows'])}\n\n")
        
        # Show stats
        self.content_text.insert(tk.END, "-" * 70 + "\n")
        self.content_text.insert(tk.END, "EXTRACTION STATISTICS\n")
        self.content_text.insert(tk.END, "-" * 70 + "\n")
        self.content_text.insert(tk.END, f"‚úì Matched patterns: {matched_count}\n")
        self.content_text.insert(tk.END, f"‚ö† Unseen patterns: {unseen_count}\n")
        self.content_text.insert(tk.END, f"üìä Total lines processed: {matched_count + unseen_count}\n")
        self.content_text.insert(tk.END, f"üìã Tables extracted: {len(all_tables)}\n")
        
        if matched_count + unseen_count > 0:
            match_rate = (matched_count / (matched_count + unseen_count)) * 100
            self.content_text.insert(tk.END, f"üìà Match rate: {match_rate:.1f}%\n")
        
        # ===== STEP 2: EXPORT TO CSV =====
        today = datetime.now()
        today_str = today.strftime("%m_%d_%y")
        filename = f"{email_data['trade_id']}_email_entities_export_{today_str}.csv"
        out_file = os.path.join(self.temp_path, filename)
        
        # Build records for CSV
        records = []
        
        # Add entity data
        if entities:
            for entity_name, vals in entities.items():
                for val in vals:
                    records.append({
                        'Trade_ID': email_data['trade_id'],
                        'Entity': entity_name,
                        'Value': val,
                        'Type': 'Entity'
                    })
        
        # Add table data
        if all_tables:
            for table in all_tables:
                for row in table['rows']:
                    row_dict = {
                        'Trade_ID': email_data['trade_id'],
                        'Type': 'Table',
                        'Headers': ', '.join(table['Headers'])
                    }
                    # Add table cells
                    for i, cell in enumerate(row):
                        row_dict[f'Col_{i+1}'] = cell
                    records.append(row_dict)
        
        # Create DataFrame
        if records:
            table_df = pd.DataFrame(records)
        else:
            # Empty DataFrame with headers
            table_df = pd.DataFrame(columns=self.selected_columns)
        
        # Save to CSV (append if exists)
        if os.path.exists(out_file):
            existing = pd.read_csv(out_file)
            all_cols = list(set(existing.columns).union(set(table_df.columns)))
            existing = existing.reindex(columns=all_cols)
            table_df = table_df.reindex(columns=all_cols)
            combined = pd.concat([existing, table_df], ignore_index=True)
            combined = combined.drop_duplicates().reset_index(drop=True)
            combined.to_csv(out_file, index=False)
        else:
            table_df.to_csv(out_file, index=False)
        
        self.content_text.insert(tk.END, f"\n‚úÖ Exported to CSV: {filename}\n")
        logger.info(f"Success - Data exported to {filename}")
        
        # ===== STEP 3: GENERATE RAG REPORT =====
        print(f"\nGenerating RAG report...")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        rag_report_path = f"{self.temp_path}/RAG_Pattern_Analysis_{timestamp}.xlsx"
        
        rag_results = self.rag_extractor.finish_and_generate_report(
            rag_report_path,
            matched_data=matched_patterns
        )
        
        self.content_text.insert(tk.END, f"üìÑ RAG report: {os.path.basename(rag_report_path)}\n")
        
        # ===== FINAL MESSAGE =====
        messagebox.showinfo("Success", 
            f"Extraction Complete!\n\n"
            f"‚úì Matched: {matched_count}\n"
            f"‚ö† Unseen: {unseen_count}\n"
            f"üìã Tables: {len(all_tables)}\n\n"
            f"CSV: {filename}\n"
            f"RAG: {os.path.basename(rag_report_path)}\n\n"
            f"Results displayed in Email Content area")
    
    except Exception as e:
        logger.error(f"Error: {str(e)}", exc_info=True)
        messagebox.showerror("Error", f"Failed: {str(e)}")


# ============================================================================
# BUTTON SETUP (Simplified)
# ============================================================================

def setup_buttons(self):
    """Setup buttons - SIMPLIFIED"""
    
    button_frame = ttk.Frame(self.main_frame)
    button_frame.grid(row=9, column=0, columnspan=8, pady=10)
    
    # Single button does everything
    ttk.Button(
        button_frame, 
        text="üîç Extract & Export", 
        command=self.extract_and_export,
        width=20
    ).pack(side=tk.LEFT, padx=5)
    
    ttk.Button(
        button_frame, 
        text="üìä Export Consolidated", 
        command=self.export_consolidated_results,
        width=20
    ).pack(side=tk.LEFT, padx=5)
    
    # Optional: Keep separate buttons if you want
    # ttk.Button(button_frame, text="Extract Only", command=lambda: self.extract_and_export()).pack(side=tk.LEFT, padx=5)
