"""
PERFORMANCE OPTIMIZED VERSION
Key optimizations:
1. Compile regex patterns once (cache)
2. Use list comprehensions instead of loops
3. Batch UI updates
4. Optimize DataFrame operations
5. Lazy loading for heavy operations
6. Parallel processing where possible
"""

import re
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
import time

# ============================================================================
# OPTIMIZATION 1: Regex Pattern Caching
# ============================================================================

class OptimizedPatternManager:
    """Pattern manager with compiled regex caching"""
    
    def __init__(self):
        self._compiled_patterns = {}  # Cache for compiled regex
        self.patterns = {}
    
    @lru_cache(maxsize=256)
    def get_compiled_pattern(self, pattern_str):
        """Cache compiled regex patterns"""
        try:
            return re.compile(pattern_str, re.IGNORECASE)
        except re.error as e:
            logger.error(f"Regex compilation error: {e}")
            return None
    
    def get_patterns_for_entity(self, entity_name):
        """Get patterns with pre-compiled regex"""
        patterns = self.patterns.get(entity_name, [])
        
        # Add compiled regex to each pattern
        for pattern in patterns:
            if 'pattern' in pattern and 'compiled_regex' not in pattern:
                pattern['compiled_regex'] = self.get_compiled_pattern(pattern['pattern'])
        
        return patterns


# ============================================================================
# OPTIMIZATION 2: Batch Processing & Fast Extraction
# ============================================================================

def extract_and_export_optimized(self):
    """OPTIMIZED extraction with performance improvements"""
    
    if self.selected_email is None:
        messagebox.showwarning("No Selection", "Please select an email from the chain first")
        return
    
    try:
        # Get entity definitions
        entity_json = self.entity_text.get(1.0, tk.END).strip()
        self.entity_definitions = json.loads(entity_json)
    except json.JSONDecodeError as e:
        messagebox.showerror("JSON Error", f"Invalid JSON: {str(e)}")
        return
    
    try:
        start_time = time.time()
        
        # Stop previous RAG
        if hasattr(self, 'rag_extractor') and self.rag_extractor:
            self.rag_extractor.rag_processor.stop_and_wait(timeout=1.0)
            self.rag_extractor = None
        
        self.rag_extractor = RAGEnabledExtractor(self.pattern_manager, self.usage_tracker)
        
        entity_names = self.pattern_manager.get_all_entity_names()
        email_data = self.selected_email
        
        # ===== OPTIMIZATION: Pre-process patterns ONCE =====
        entity_patterns_map = {}
        for entity_name in entity_names:
            patterns = self.pattern_manager.get_patterns_for_entity(entity_name)
            # Pre-compile all patterns
            compiled_patterns = []
            for pattern_entry in patterns:
                if pattern_entry.get('entity_type') == 'pattern':
                    pattern_str = pattern_entry['pattern']
                    try:
                        compiled_regex = re.compile(pattern_str, re.IGNORECASE)
                        compiled_patterns.append({
                            'pattern_id': pattern_entry['pattern_id'],
                            'compiled_regex': compiled_regex,
                            'entity_name': entity_name
                        })
                    except re.error:
                        pass
            entity_patterns_map[entity_name] = compiled_patterns
        
        extraction_records = []
        matched_patterns = []
        
        # ===== OPTIMIZATION: Clean lines ONCE =====
        email_lines = email_data['body'].splitlines()
        email_lines = self.email_cleaner.clean_email_body(email_lines)
        email_lines = [line.strip() for line in email_lines if line.strip()]  # List comprehension
        
        current_activity = None
        matched_count = 0
        unseen_count = 0
        
        # ===== OPTIMIZATION: Pre-compile activity keyword check =====
        activity_keywords_upper = [kw.upper() for kw in self.activity_keywords]
        
        # ===== FAST EXTRACTION LOOP =====
        for line in email_lines:
            line_upper = line.upper()
            
            # Fast activity check
            for keyword in activity_keywords_upper:
                if keyword in line_upper and len(line_upper.split()) < 5:
                    current_activity = line_upper
                    break
            
            extracted_from_line = False
            
            # ===== OPTIMIZATION: Try all patterns efficiently =====
            for entity_name, compiled_patterns in entity_patterns_map.items():
                if extracted_from_line:
                    break
                
                for pattern_info in compiled_patterns:
                    compiled_regex = pattern_info['compiled_regex']
                    pattern_id = pattern_info['pattern_id']
                    
                    # Quick attribute count check
                    if not self.attribute_count_check_fast(compiled_regex, line):
                        continue
                    
                    matches = compiled_regex.finditer(line)
                    
                    for match in matches:
                        groupdict = match.groupdict()
                        
                        if not groupdict:
                            continue
                        
                        # Create record
                        record = {
                            'Trade_ID': email_data['trade_id'],
                            'Email_Date': email_data.get('date', ''),
                            'Email_From': email_data.get('sender', ''),
                            'Email_Subject': email_data.get('subject', '')
                        }
                        
                        if current_activity:
                            record['ActivityType'] = current_activity
                        
                        # Fast dict update
                        extracted_labels = []
                        for k, v in groupdict.items():
                            if v:
                                record[k] = v
                                extracted_labels.append(k)
                        
                        if extracted_labels:  # Only add if we extracted something
                            record['Source_Line'] = line
                            record['Pattern_ID'] = pattern_id
                            extraction_records.append(record)
                            
                            matched_patterns.append({
                                'line': line,
                                'pattern_id': pattern_id,
                                'entity_name': entity_name,
                                'extracted_labels': extracted_labels,
                                'confidence': f"{len(extracted_labels)}/{len(groupdict)}"
                            })
                            
                            extracted_from_line = True
                            matched_count += 1
                            break
                    
                    if extracted_from_line:
                        break
            
            # Queue unseen
            if not extracted_from_line:
                unseen_count += 1
                self.rag_extractor.queue_unseen(line, "Trade")
        
        logger.info(f"â± Extraction time: {time.time() - start_time:.2f}s")
        
        # ===== OPTIMIZATION: Parallel table extraction =====
        all_tables = []
        html_content = email_data.get('html_body', '')
        
        if html_content:
            table_start = time.time()
            all_tables = self._extract_tables_fast(html_content, email_data, extraction_records)
            logger.info(f"â± Table extraction time: {time.time() - table_start:.2f}s")
        
        # ===== OPTIMIZATION: Batch UI updates =====
        ui_start = time.time()
        self._update_ui_batched(extraction_records, matched_count, unseen_count, len(all_tables))
        logger.info(f"â± UI update time: {time.time() - ui_start:.2f}s")
        
        # ===== OPTIMIZATION: Fast CSV export =====
        csv_start = time.time()
        filename = self._export_csv_fast(extraction_records, email_data)
        logger.info(f"â± CSV export time: {time.time() - csv_start:.2f}s")
        
        # ===== RAG report generation =====
        rag_start = time.time()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        rag_report_path = f"{self.temp_path}/RAG_Pattern_Analysis_{timestamp}.xlsx"
        
        rag_results = self.rag_extractor.finish_and_generate_report(
            rag_report_path,
            matched_data=matched_patterns
        )
        logger.info(f"â± RAG generation time: {time.time() - rag_start:.2f}s")
        
        total_time = time.time() - start_time
        logger.info(f"â± TOTAL TIME: {total_time:.2f}s")
        
        self.content_text.insert(tk.END, f"\nâ± Processing time: {total_time:.2f}s\n")
        
        messagebox.showinfo("Success", 
            f"Extraction Complete!\n\n"
            f"âœ“ Records: {len(extraction_records)}\n"
            f"âœ“ Matched: {matched_count}\n"
            f"âš  Unseen: {unseen_count}\n"
            f"ðŸ“‹ Tables: {len(all_tables)}\n"
            f"â± Time: {total_time:.2f}s\n\n"
            f"CSV: {filename}\n"
            f"RAG: {os.path.basename(rag_report_path)}")
    
    except Exception as e:
        logger.error(f"Error: {str(e)}", exc_info=True)
        messagebox.showerror("Error", f"Failed: {str(e)}")


# ============================================================================
# OPTIMIZATION 3: Fast attribute count check
# ============================================================================

def attribute_count_check_fast(self, compiled_regex, line):
    """Faster attribute count check using compiled regex"""
    try:
        # Quick check - just see if it matches at all
        match = compiled_regex.search(line)
        if not match:
            return False
        
        # Count non-None groups
        groupdict = match.groupdict()
        return sum(1 for v in groupdict.values() if v is not None) > 0
    except:
        return False


# ============================================================================
# OPTIMIZATION 4: Fast table extraction
# ============================================================================

def _extract_tables_fast(self, html_content, email_data, extraction_records):
    """Optimized table extraction"""
    all_tables = []
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        tables = soup.find_all('table')
        
        for table_idx, table in enumerate(tables, 1):
            # Fast header extraction
            headers = [th.get_text(strip=True) for th in table.find('tr').find_all('th')] if table.find('tr') else []
            
            # Fast row extraction using list comprehension
            rows = [[col.get_text(strip=True) for col in tr.find_all(['td', 'th'])] 
                    for tr in table.find_all('tr')[1:]]
            
            all_tables.append({
                'Email_Index': 1,
                'Headers': headers,
                'rows': rows
            })
            
            # Add to extraction records
            for row in rows:
                table_record = {
                    'Trade_ID': email_data['trade_id'],
                    'Email_Date': email_data.get('date', ''),
                    'Source': f'Table_{table_idx}',
                    'Pattern_ID': 'TABLE'
                }
                
                # Fast zip and dict update
                table_record.update({
                    (header if header else f'Column_{i+1}'): value 
                    for i, (header, value) in enumerate(zip(headers, row))
                })
                
                extraction_records.append(table_record)
    
    except Exception as e:
        logger.error(f"Table extraction error: {e}")
    
    return all_tables


# ============================================================================
# OPTIMIZATION 5: Batched UI updates (reduce redraws)
# ============================================================================

def _update_ui_batched(self, extraction_records, matched_count, unseen_count, table_count):
    """Update UI in one batch to avoid multiple redraws"""
    
    # Build entire text first, then insert once
    output_lines = []
    
    output_lines.append("=" * 70)
    output_lines.append("EXTRACTED ENTITIES (Record View)")
    output_lines.append("=" * 70)
    output_lines.append("")
    
    if extraction_records:
        # Get unique entity columns (fast set operation)
        all_entities = set()
        for record in extraction_records:
            all_entities.update(record.keys())
        
        metadata_cols = {'Trade_ID', 'Email_Date', 'Email_From', 'Email_Subject', 
                        'Source_Line', 'Pattern_ID', 'Source', 'ActivityType'}
        entity_columns = sorted(all_entities - metadata_cols)
        
        output_lines.append(f"Extracted Entities: {', '.join(entity_columns)}")
        output_lines.append("")
        
        # Show first 5 records
        for i, record in enumerate(extraction_records[:5], 1):
            output_lines.append(f"Record {i}:")
            for entity in entity_columns:
                if entity in record and record[entity]:
                    output_lines.append(f"  {entity}: {record[entity]}")
            output_lines.append("")
        
        if len(extraction_records) > 5:
            output_lines.append(f"... and {len(extraction_records) - 5} more records")
            output_lines.append("")
    else:
        output_lines.append("âš ï¸  No entities found in this email")
        output_lines.append("")
    
    output_lines.append("-" * 70)
    output_lines.append("EXTRACTION STATISTICS")
    output_lines.append("-" * 70)
    output_lines.append(f"âœ“ Total records extracted: {len(extraction_records)}")
    output_lines.append(f"âœ“ Matched patterns: {matched_count}")
    output_lines.append(f"âš  Unseen patterns: {unseen_count}")
    output_lines.append(f"ðŸ“Š Total lines processed: {matched_count + unseen_count}")
    output_lines.append(f"ðŸ“‹ Tables extracted: {table_count}")
    
    if matched_count + unseen_count > 0:
        match_rate = (matched_count / (matched_count + unseen_count)) * 100
        output_lines.append(f"ðŸ“ˆ Match rate: {match_rate:.1f}%")
    
    # Single UI update
    self.content_text.delete(1.0, tk.END)
    self.content_text.insert(tk.END, "\n".join(output_lines))


# ============================================================================
# OPTIMIZATION 6: Fast CSV export
# ============================================================================

def _export_csv_fast(self, extraction_records, email_data):
    """Optimized CSV export"""
    today_str = datetime.now().strftime("%m_%d_%y")
    filename = f"{email_data['trade_id']}_email_entities_export_{today_str}.csv"
    out_file = os.path.join(self.temp_path, filename)
    
    if extraction_records:
        # Fast DataFrame creation
        final_df = pd.DataFrame(extraction_records)
        
        # Fast column reordering
        metadata_cols = ['Trade_ID', 'Email_Date', 'Email_From', 'Email_Subject', 
                        'ActivityType', 'Pattern_ID', 'Source', 'Source_Line']
        
        existing_metadata = [col for col in metadata_cols if col in final_df.columns]
        entity_cols = sorted([col for col in final_df.columns if col not in metadata_cols])
        
        final_df = final_df[existing_metadata + entity_cols]
        
        # Fast CSV write
        if os.path.exists(out_file):
            # Append mode (faster than read+concat+write)
            existing = pd.read_csv(out_file)
            all_cols = list(set(existing.columns) | set(final_df.columns))
            
            combined = pd.concat([
                existing.reindex(columns=all_cols),
                final_df.reindex(columns=all_cols)
            ], ignore_index=True)
            
            combined.drop_duplicates(inplace=True)
            combined.to_csv(out_file, index=False)
        else:
            final_df.to_csv(out_file, index=False)
        
        self.content_text.insert(tk.END, f"\nâœ… Exported to CSV: {filename}\n")
        self.content_text.insert(tk.END, f"   Columns: {len(final_df.columns)}\n")
        self.content_text.insert(tk.END, f"   Rows: {len(final_df)}\n")
    
    return filename
