def export_consolidated_results(self):
    """
    Export consolidated results by appending all unique temp trade CSVs
    to a single Excel file, with PROPER duplicate handling
    """
    try:
        temp_df = None
        today = datetime.now()
        today_str = today.strftime("%m_%d_%y")
        
        # Find all CSV files
        csv_files = glob.glob(f"{self.temp_path}/*.csv")
        sheet_name = "Consolidated_Report"
        consolidated_file = f"{self.out_path}/{today_str}_{sheet_name}.xlsx"
        lock_path = consolidated_file + '.lock'
        ws_rows = []
        
        # Use file lock
        with FileLock(lock_path, timeout=60):
            for file in csv_files:
                base_name = os.path.basename(file)
                trade_id = base_name.split("_")[0]
                rows_found = False
                
                with open(file, newline="", encoding="utf-8") as f:
                    reader = csv.DictReader(f)
                    csv_headers = list(reader.fieldnames) if reader.fieldnames else []
                    expected_headers = self.static_headers[1:]
                    required_sub_headers = self.static_headers[20:]
                    
                    # Check if required headers exist
                    if all(header in csv_headers for header in required_sub_headers):
                        # Append data rows if all required headers exist
                        for row in reader:
                            out_row = [trade_id]
                            for header in expected_headers:
                                out_row.append(row.get(header, ""))
                            ws_rows.append(out_row)
                        rows_found = True
                    
                    elif any(header in csv_headers for header in expected_headers[1:]):
                        # Partial header mismatch
                        for row in reader:
                            out_row = [trade_id]
                            for header in expected_headers:
                                out_row.append(row.get(header, ""))
                            ws_rows.append(out_row)
                        rows_found = True
                    
                    else:
                        # No match
                        if len(csv_headers) == 0:
                            out_row = [f"{trade_id} - Found In Mail - No Entity Found"] + [""] * (len(self.static_headers) - 1)
                        else:
                            out_row = [f"{trade_id} - Partial header mismatch - Check file: {base_name}"] + [""] * (len(self.static_headers) - 1)
                        ws_rows.append(out_row)
                        rows_found = True
                
                f.close()
                
                if not rows_found:
                    out_row = [f"{trade_id} - Not Found In Mail"] + [""] * (len(self.static_headers) - 1)
                    ws_rows.append(out_row)
                    os.remove(file)
            
            # Build new dataframe
            df_new = pd.DataFrame(ws_rows, columns=self.static_headers)
            
            # ===== FIX: PROPER DUPLICATE HANDLING =====
            if os.path.isfile(consolidated_file):
                # File exists - need to preserve RAG sheets and avoid duplicates
                logger.info(f"Consolidated file exists - preserving RAG sheets")
                
                # Read existing Sheet1 data
                df_existing = pd.read_excel(consolidated_file, sheet_name=sheet_name)
                
                # ===== CREATE COMPOSITE KEY FOR DEDUPLICATION =====
                # Define columns that make a record unique
                key_columns = ['Trade_ID']  # Add more if needed
                
                # Add composite key to both dataframes
                if not df_existing.empty:
                    # Create a hash of key columns for existing data
                    df_existing['_dedup_key'] = df_existing[key_columns].apply(
                        lambda row: '_'.join(row.values.astype(str)), axis=1
                    )
                
                if not df_new.empty:
                    # Create same hash for new data
                    df_new['_dedup_key'] = df_new[key_columns].apply(
                        lambda row: '_'.join(row.values.astype(str)), axis=1
                    )
                    
                    # ===== FILTER OUT DUPLICATES =====
                    # Only keep new records that don't exist in existing data
                    existing_keys = set(df_existing['_dedup_key'].values) if not df_existing.empty else set()
                    df_new_filtered = df_new[~df_new['_dedup_key'].isin(existing_keys)]
                    
                    logger.info(f"üìä Existing records: {len(df_existing)}")
                    logger.info(f"üìä New records (before dedup): {len(df_new)}")
                    logger.info(f"üìä New records (after dedup): {len(df_new_filtered)}")
                    logger.info(f"üóëÔ∏è Duplicates removed: {len(df_new) - len(df_new_filtered)}")
                    
                    # Drop the temporary key column
                    df_existing = df_existing.drop('_dedup_key', axis=1)
                    df_new_filtered = df_new_filtered.drop('_dedup_key', axis=1)
                    
                    # Combine existing + new (deduplicated)
                    df_combined = pd.concat([df_existing, df_new_filtered], ignore_index=True)
                else:
                    df_combined = df_existing
                
                # Load workbook to preserve other sheets
                wb = load_workbook(consolidated_file)
                
                # Save all sheet names EXCEPT Sheet1
                other_sheets = {}
                for ws_name in wb.sheetnames:
                    if ws_name != sheet_name:
                        # Read data from other sheets
                        ws = wb[ws_name]
                        sheet_data = []
                        for row in ws.iter_rows(values_only=True):
                            sheet_data.append(row)
                        other_sheets[ws_name] = sheet_data
                
                wb.close()
                
                # Write combined data to Sheet1
                df_combined.to_excel(consolidated_file, sheet_name=sheet_name, index=False)
                
                # Restore other sheets
                if other_sheets:
                    wb = load_workbook(consolidated_file)
                    for ws_name, sheet_data in other_sheets.items():
                        if ws_name not in wb.sheetnames:
                            ws = wb.create_sheet(ws_name)
                        else:
                            ws = wb[ws_name]
                        
                        # Write data back
                        for row_idx, row in enumerate(sheet_data, 1):
                            for col_idx, value in enumerate(row, 1):
                                ws.cell(row=row_idx, column=col_idx, value=value)
                    
                    wb.save(consolidated_file)
                    wb.close()
                    logger.info(f"‚úÖ Preserved {len(other_sheets)} RAG sheets")
            
            else:
                # New file - just write Sheet1
                df_new.to_excel(consolidated_file, sheet_name=sheet_name, index=False)
                logger.info("‚úÖ Created new consolidated file")
            
            # Display summary
            if not df_new.empty:
                self.content_text.delete(1.0, tk.END)
                self.content_text.insert(tk.END, f"{df_new}\n")
            else:
                self.content_text.insert(tk.END, "No Data found in this email")
        
        logger.info(f"Success - Consolidated Report File Generated")
        
        # ===== NOW APPEND RAG DATA (SHEETS ALREADY PRESERVED) =====
        self._append_rag_to_consolidated(consolidated_file)
        
        # Copy files and cleanup
        self.copy_files(self.temp_path, self.out_path)
        
        messagebox.showinfo("Success", f"Consolidated Report File Generated: {self.out_path}")
    
    except Exception as e:
        logger.error(f"Consolidation error: {str(e)}", exc_info=True)
        messagebox.showerror("Error", f"Failed to consolidate: {str(e)}")
