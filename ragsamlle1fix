"""
OPTIMIZED: Simple & Efficient Extraction
- Uses already-cleaned body from thread parser (no extra processing)
- No signature removal needed (thread parser handles it)
- Keeps your existing extraction logic 100% intact
- Lightweight RAG background processing
"""

import re
import logging
import threading
import queue
from typing import Dict, List
from collections import defaultdict
from datetime import datetime
import pandas as pd

logger = logging.getLogger(__name__)


# ============================================================================
# SIMPLE RAG BACKGROUND PROCESSOR
# ============================================================================

class SimpleRAGProcessor:
    """Lightweight RAG - just tracks unseen patterns"""
    
    def __init__(self, pattern_manager, knowledge_base):
        self.pattern_manager = pattern_manager
        self.knowledge_base = knowledge_base
        
        # Extract config patterns once
        self.config_patterns = self._extract_config_patterns()
        logger.info(f"Loaded {len(self.config_patterns)} patterns")
        
        # Queue and storage
        self.unseen_queue = queue.Queue()
        self.unseen_data = defaultdict(list)
        
        # Thread control
        self.is_running = False
        self.processor_thread = None
    
    def _extract_config_patterns(self) -> Dict:
        """Extract all patterns from config"""
        patterns = {}
        
        for entity_name in self.pattern_manager.get_all_entity_names():
            entity_patterns = self.pattern_manager.get_patterns_for_entity(entity_name)
            
            for pattern_info in entity_patterns:
                pattern_id = pattern_info.get('pattern_id')
                pattern_str = pattern_info.get('pattern', '')
                
                if pattern_id and pattern_str:
                    try:
                        patterns[pattern_id] = {
                            'pattern': pattern_str,
                            'entity': entity_name,
                            'compiled': re.compile(pattern_str, re.IGNORECASE | re.MULTILINE | re.DOTALL)
                        }
                    except Exception as e:
                        logger.debug(f"Pattern {pattern_id} error: {e}")
        
        return patterns
    
    def start(self):
        """Start background thread"""
        if self.is_running:
            return
        
        self.is_running = True
        self.processor_thread = threading.Thread(
            target=self._process_queue,
            daemon=True,
            name="RAG-Processor"
        )
        self.processor_thread.start()
        logger.info("RAG processor started")
    
    def add_unseen_pattern(self, line: str, entity_name: str):
        """Queue unseen line"""
        if not line or not line.strip():
            return
        
        self.unseen_queue.put({
            'line': line.strip(),
            'entity_name': entity_name,
            'timestamp': datetime.now()
        })
    
    def _process_queue(self):
        """Background thread: process unseen patterns"""
        while self.is_running:
            try:
                item = self.unseen_queue.get(timeout=0.5)
                
                line = item['line']
                entity_name = item['entity_name']
                
                # Find closest matching patterns
                closest = self._find_closest_patterns(line)
                
                # Store analysis
                if closest:
                    self.unseen_data[entity_name].append({
                        'line': line,
                        'closest_patterns': closest,
                        'timestamp': item['timestamp']
                    })
                
                self.unseen_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                logger.debug(f"RAG error: {e}")
    
    def _find_closest_patterns(self, line: str, top_k: int = 3) -> List[Dict]:
        """Find closest matching patterns"""
        matches = []
        
        for pattern_id, pattern_info in self.config_patterns.items():
            try:
                match = pattern_info['compiled'].search(line)
                
                if match:
                    # Extract data
                    extracted = {k: v for k, v in match.groupdict().items() if v}
                    
                    matches.append({
                        'pattern_id': pattern_id,
                        'entity_name': pattern_info['entity'],
                        'pattern': pattern_info['pattern'][:100],
                        'extracted': extracted,
                        'confidence': len(extracted) / max(len(match.groupdict()), 1)
                    })
            
            except Exception as e:
                logger.debug(f"Match error: {e}")
        
        matches.sort(key=lambda x: x['confidence'], reverse=True)
        return matches[:top_k]
    
    def stop_and_wait(self, timeout: float = 3.0):
        """Stop processor"""
        logger.info("Stopping RAG processor...")
        
        try:
            self.unseen_queue.join()
        except:
            pass
        
        self.is_running = False
        
        if self.processor_thread and self.processor_thread.is_alive():
            self.processor_thread.join(timeout=timeout)
        
        logger.info(f"RAG stopped. Unseen entries: {sum(len(v) for v in self.unseen_data.values())}")
    
    def get_results(self) -> Dict:
        """Get unseen data"""
        return dict(self.unseen_data)
    
    def save_rag_report(self, output_path: str):
        """Save RAG report"""
        logger.info(f"Saving: {output_path}")
        
        results = self.get_results()
        
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            
            # Sheet 1: Config Patterns
            patterns_data = [
                {
                    'Pattern ID': pattern_id,
                    'Entity': pattern_info['entity'],
                    'Pattern': pattern_info['pattern'][:150]
                }
                for pattern_id, pattern_info in self.config_patterns.items()
            ]
            
            if patterns_data:
                pd.DataFrame(patterns_data).to_excel(writer, sheet_name='Config Patterns', index=False)
            
            # Sheet 2: Unseen Patterns Detected
            unseen_data = []
            for entity, analyses in results.items():
                for analysis in analyses:
                    unseen_data.append({
                        'Entity': entity,
                        'Unseen Line': analysis['line'][:80],
                        'Full Line': analysis['line'],
                    })
            
            if unseen_data:
                pd.DataFrame(unseen_data).to_excel(writer, sheet_name='Unseen Patterns', index=False)
            
            # Sheet 3: Suggestions
            suggestions_data = []
            for entity, analyses in results.items():
                for analysis in analyses:
                    for closest in analysis['closest_patterns']:
                        suggestions_data.append({
                            'Unseen Line': analysis['line'][:60],
                            'Suggested Pattern': closest['pattern_id'],
                            'Confidence': f"{closest['confidence']:.1%}",
                            'Extracted': str(closest['extracted'])[:100]
                        })
            
            if suggestions_data:
                pd.DataFrame(suggestions_data).to_excel(writer, sheet_name='Suggestions', index=False)
            
            # Sheet 4: Summary
            summary_data = [
                {'Metric': 'Total Config Patterns', 'Value': len(self.config_patterns)},
                {'Metric': 'Unseen Patterns Found', 'Value': sum(len(v) for v in results.values())},
            ]
            
            pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)


# ============================================================================
# RAG WRAPPER
# ============================================================================

class RAGEnabledExtractor:
    """RAG wrapper"""
    
    def __init__(self, pattern_manager, tracker):
        self.pattern_manager = pattern_manager
        self.tracker = tracker
        
        try:
            from offline_rag import OfflinePatternKnowledgeBase
            knowledge_base = OfflinePatternKnowledgeBase(pattern_manager.config_file)
        except:
            knowledge_base = None
        
        self.rag_processor = SimpleRAGProcessor(pattern_manager, knowledge_base)
        self.rag_processor.start()
    
    def queue_unseen(self, line: str, entity_name: str):
        """Queue unseen line"""
        self.rag_processor.add_unseen_pattern(line, entity_name)
    
    def finish_and_generate_report(self, report_path: str = None):
        """Finish and generate report"""
        logger.info("Finalizing RAG analysis...")
        self.rag_processor.stop_and_wait(timeout=3.0)
        
        if report_path:
            self.rag_processor.save_rag_report(report_path)
        
        return self.rag_processor.get_results()


# ============================================================================
# MAIN EXTRACTION - KEEP YOUR EXISTING FLOW 100%
# ============================================================================

def extract_entities(self):
    """Extract entities - YOUR EXISTING FLOW PRESERVED"""
    
    if self.selected_email is None:
        messagebox.showwarning("No Selection", "Please select an email")
        return
    
    try:
        entity_json = self.entity_text.get(1.0, tk.END).strip()
        self.entity_definitions = json.loads(entity_json)
    except json.JSONDecodeError as e:
        messagebox.showerror("JSON Error", f"Invalid JSON: {str(e)}")
        return
    
    try:
        self.clear_results()
        
        # Initialize RAG once
        if not hasattr(self, 'rag_extractor') or self.rag_extractor is None:
            self.rag_extractor = RAGEnabledExtractor(self.pattern_manager, self.usage_tracker)
        
        # Get email data
        email_data = self.selected_email
        entity_names = self.pattern_manager.get_all_entity_names()
        results = {}
        entities = {}
        
        logger.info(f"Processing: {email_data['subject']}")
        print(f"\n[1/2] Extracting entities...")
        
        # ===== USE BODY AS-IS (already cleaned by thread parser) =====
        # The 'body' key already has threads/signatures removed
        email_lines = email_data['body'].splitlines()
        email_lines = list(filter(None, email_lines))
        
        current_activity = None
        matched_count = 0
        unseen_count = 0
        
        for line in email_lines:
            line = line.strip()
            if not line:
                continue
            
            # Check for activity keywords
            line_upper = line.upper()
            for keyword in self.activity_keywords:
                if keyword in line_upper:
                    current_activity = line_upper
                    break
            
            # ===== YOUR EXISTING EXTRACTION LOGIC =====
            extracted_from_line = False
            
            for entity_name in entity_names:
                patterns = self.pattern_manager.get_patterns_for_entity(entity_name)
                
                result = {
                    'value': None,
                    'pattern_id': None,
                    'entity_name': entity_name
                }
                
                for pattern_entry in patterns:
                    entity_type = pattern_entry.get('entity_type', 'pattern')
                    
                    if entity_type == "pattern":
                        pattern_id = pattern_entry['pattern_id']
                        pattern_regex = pattern_entry['pattern']
                        
                        self.usage_tracker.record_attempt(pattern_id)
                        
                        try:
                            # Your existing pattern check
                            if self.attribute_count_check(pattern_regex, line):
                                regex = re.compile(pattern_regex, re.IGNORECASE)
                                matches = regex.finditer(line)
                                
                                for match in matches:
                                    groupdict = match.groupdict()
                                    
                                    if current_activity:
                                        groupdict['ActivityType'] = current_activity
                                    
                                    for k, v in groupdict.items():
                                        if v:
                                            entities.setdefault(k, []).append(f"{v}")
                                    
                                    self.usage_tracker.record_match(pattern_id, line)
                                    result['value'] = line
                                    result['pattern_id'] = pattern_id
                                    extracted_from_line = True
                                    matched_count += 1
                                    break
                        
                        except re.error as regex_error:
                            logger.error(f"Regex error {pattern_id}: {regex_error}")
                    
                    elif entity_type == "gazetteer":
                        # Your gazetteer extraction
                        entity_def = self.entity_definitions.get(entity_name, {})
                        gazetteer_vals = entity_def.get("values", [])
                        subject_vals = self.extract_with_gazetteer(email_data['subject'], gazetteer_vals)
                        body_vals = self.extract_with_gazetteer(line, gazetteer_vals)
                        
                        all_vals = subject_vals + body_vals
                        
                        if all_vals:
                            entities[entity_name] = all_vals
                            extracted_from_line = True
                
                results[entity_name] = result
            
            # ===== If no pattern matched, queue for RAG =====
            if not extracted_from_line:
                unseen_count += 1
                self.rag_extractor.queue_unseen(line, "Trade")
        
        logger.info(f"Extraction: {matched_count} matched, {unseen_count} unseen")
        
        # Display results
        self.entity_result_text.delete(1.0, tk.END)
        
        if entities:
            for entity_name, vals in entities.items():
                self.entity_result_text.insert(tk.END, f"{entity_name}:\n")
                for v in vals:
                    self.entity_result_text.insert(tk.END, f"  - {v}\n")
        else:
            self.entity_result_text.insert(tk.END, "No entities found")
        
        self.finish_extraction_with_mi()
        
        # Generate RAG report
        print(f"\n[2/2] Generating RAG report...")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        rag_report_path = f"RAG_Pattern_Analysis_{timestamp}.xlsx"
        
        rag_results = self.rag_extractor.finish_and_generate_report(rag_report_path)
        
        print(f"âœ“ Complete!")
        print(f"  Matched: {matched_count}")
        print(f"  Unseen: {unseen_count}")
        print(f"  Report: {rag_report_path}")
        
        messagebox.showinfo("Success", 
            f"Extraction complete!\n\n"
            f"Matched: {matched_count}\n"
            f"Unseen: {unseen_count}\n\n"
            f"Report: {rag_report_path}")
    
    except Exception as e:
        logger.error(f"Error: {str(e)}", exc_info=True)
        messagebox.showerror("Error", f"Failed: {str(e)}")
