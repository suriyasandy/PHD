def _append_rag_to_consolidated(self, consolidated_file, str):
    """
    Append RAG Report data to consolidated file (sheets)
    FIX: Properly handle existing sheets and append without recreating
    """
    try:
        # Find latest RAG report in temp_path
        rag_files = glob.glob(f"{self.temp_path}/RAG_Pattern_Analysis_*.xlsx")
        
        if not rag_files:
            logger.info("No RAG report found - skipping RAG consolidation")
            return
        
        # Get most recent RAG file
        latest_rag = max(rag_files, key=os.path.getmtime)
        logger.info(f"Found RAG report: {os.path.basename(latest_rag)}")
        
        # Extract timestamp from filename
        rag_filename = os.path.basename(latest_rag)
        timestamp = rag_filename.replace("RAG_Pattern_Analysis_", "").replace(".xlsx", "")
        
        # RAG sheet names
        rag_sheets = [
            'Unseen Patterns',
            'Matched Patterns',
            'Config Patterns',
            'Suggestions',
            'Summary'
        ]
        
        # Load consolidated file (NOT read-only)
        consolidated_wb = load_workbook(consolidated_file)
        
        # Load RAG file (read-only is OK here)
        rag_wb = load_workbook(latest_rag, read_only=True)
        
        sheets_processed = 0
        sheets_skipped = 0
        
        try:
            for sheet_name in rag_sheets:
                if sheet_name not in rag_wb.sheetnames:
                    logger.debug(f"** Sheet '{sheet_name}' not in RAG - skipping")
                    sheets_skipped += 1
                    continue
                
                rag_sheet = rag_wb[sheet_name]
                
                # ===== FIX #1: Check if sheet already exists =====
                if sheet_name in consolidated_wb.sheetnames:
                    consolidated_sheet = consolidated_wb[sheet_name]
                    logger.info(f"** {sheet_name}: Sheet exists (appending)")
                else:
                    # Create sheet if doesn't exist
                    consolidated_sheet = consolidated_wb.create_sheet(sheet_name)
                    logger.info(f"** {sheet_name}: Sheet created (new)")
                
                # Read data from RAG sheet
                data = []
                headers = []
                
                for idx, row in enumerate(rag_sheet.iter_rows(values_only=True)):
                    if idx == 0:
                        headers = list(row)
                    else:
                        if any(cell is not None and str(cell).strip() for cell in row):
                            data.append(list(row))
                
                # Skip if no data
                if not 
                    logger.info(f"** No data - SKIPPED")
                    sheets_skipped += 1
                    continue
                
                # ===== FIX #2: Check if sheet has headers =====
                current_max_row = consolidated_sheet.max_row
                
                if current_max_row == 1:
                    # Check if row 1 has any data
                    has_header = False
                    for cell in consolidated_sheet[1]:
                        if cell.value is not None:
                            has_header = True
                            break
                    
                    if not has_header:
                        # Write headers
                        for col_idx, header in enumerate(headers, 1):
                            consolidated_sheet.cell(row=1, column=col_idx, value=header)
                        start_row = 2
                        logger.info(f"** Headers written to row 1")
                    else:
                        # Headers already exist
                        start_row = 2
                        logger.info(f"** Headers already exist")
                else:
                    # Sheet has data, append after last row
                    start_row = current_max_row + 1
                
                # ===== FIX #3: Create dataframe and append =====
                df = pd.DataFrame(data, columns=headers)
                
                # Add metadata
                if 'Source File' not in df.columns:
                    df['Source File'] = rag_filename
                if 'Timestamp' not in df.columns:
                    df['Timestamp'] = timestamp
                
                # Append rows
                for row_data, row in enumerate(df.itertuples(index=False), start_row):
                    for col_idx, value in enumerate(row, 1):
                        consolidated_sheet.cell(row=start_row + row_data - start_row, column=col_idx, value=value)
                
                logger.info(f"Appended {len(data)} rows (rows {start_row}–{start_row + len(data) - 1})")
                sheets_processed += 1
            
            # ===== NEW: DEDUPLICATE SPECIFIC SHEETS =====
            self._deduplicate_sheets(consolidated_wb)
            
            # ===== FIX #4: Save PROPERLY =====
            try:
                consolidated_wb.save(consolidated_file)
                logger.info(f"File saved successfully: {consolidated_file}")
                consolidated_wb.close()
                rag_wb.close()
                logger.info(f"File saved (retry successful)")
            except Exception as save_error:
                logger.error(f"Save error: {save_error}")
                raise
                
        except Exception as inner_error:
            logger.error(f"Inner processing error: {inner_error}")
            consolidated_wb.close()
            rag_wb.close()
            raise
            
    except Exception as e:
        logger.error(f"RAG consolidation failed: {e}")
        raise


def _deduplicate_sheets(self, consolidated_wb):
    """
    Remove duplicates from specific sheets after consolidation
    Uses pure openpyxl for efficiency
    """
    duplicate_rules = {
        "Unseen Patterns": ["Query_Trade_ID", "Full Line"],
        "Matched Patterns": ["Query_Trade_ID", "Full Line", "Pattern_ID"]
    }
    
    logger.info("[Dedup] Starting deduplication process...")
    
    for sheet_name, dedup_cols in duplicate_rules.items():
        if sheet_name not in consolidated_wb.sheetnames:
            logger.info(f"[Dedup] Sheet '{sheet_name}' not found – skipping")
            continue

        sheet = consolidated_wb[sheet_name]
        
        if sheet.max_row <= 1:
            logger.info(f"[Dedup] Sheet '{sheet_name}' empty – skipping")
            continue

        # Get headers and find column indices
        headers = [cell.value for cell in sheet[1]]
        dedup_col_indices = []
        
        for col in dedup_cols:
            if col in headers:
                dedup_col_indices.append(headers.index(col))
        
        if not dedup_col_indices:
            logger.warning(f"[Dedup] None of the dedup columns exist in sheet '{sheet_name}'")
            logger.warning(f"[Dedup] Available columns: {headers}")
            continue

        # Track seen combinations and rows to delete
        seen = set()
        rows_to_delete = []
        
        for row_idx in range(2, sheet.max_row + 1):  # Start from row 2 (skip header)
            # Build tuple of values from dedup columns
            key_values = tuple(
                sheet.cell(row=row_idx, column=idx + 1).value 
                for idx in dedup_col_indices
            )
            
            if key_values in seen:
                rows_to_delete.append(row_idx)
            else:
                seen.add(key_values)
        
        # Delete rows in reverse order (important!)
        for row_idx in reversed(rows_to_delete):
            sheet.delete_rows(row_idx, 1)
        
        logger.info(f"[Dedup] Sheet '{sheet_name}': Removed {len(rows_to_delete)} duplicates")
    
    logger.info("[Dedup] Completed cleanup for all sheets")
